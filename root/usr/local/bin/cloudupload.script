#!/bin/bash
. "util"

check_cloud_decrypt

newSize=0
oldSize=0

# Generate filelist and iterate through it...
find ${local_decrypt_dir} -type f -print0 | xargs -0 stat --format '%Y :%y %n' | cut -d: -f2- | sort -n | awk '{$1=$2=$3=""; print $0}' |
while read -r n; do

    # Find the pathname relative to the root of your remote and store filename
    filename="$(echo "$n" | sed -e s@"${local_decrypt_dir}"@@)"
    destpath="$(dirname "$n" | sed -e s@"${local_decrypt_dir}"@@)"

    # Skip hidden or partial files.
    case "$n" in
        (*.partial~) continue ;;
        (*_HIDDEN~) continue ;;
        (*.QTFS) continue ;;
        (*.unionfs-fuse*) continue ;;
        (*.DS_STORE) continue ;;
        (*.download*) continue ;;
    esac

    if [[ -f "${cloud_decrypt_dir}${filename}" ]]; then
        continue
    fi

    # If file is opened by another process, wait until it isn't.
    while [[ "$(lsof "$n" > /dev/null 2>&1)" ]] \
 || [[ "$(lsof "${local_decrypt_dir}/${n}" > /dev/null 2>&1)" ]] \
 || [[ "$(lsof "${local_media_dir}/${n}" > /dev/null 2>&1)" ]]; do
        warn "File '${n}' in use. Retrying in 10 seconds."
        sleep 10
    done

    # Copy file to remote destination[s], retaining path
    info "Uploading File: '${local_decrypt_dir}${filename}' to: '${cloud_decrypt_endpoint}${destpath}'"

    rclone copy ${rclone_options} "$n" "${cloud_decrypt_endpoint}${destpath}" > /dev/null 2>&1

    fileSize=$(du -sb "$n" | awk '{print $1}')
    newSize=$((newSize + fileSize))
    sizeInMb=$((newSize / 1000 / 1000))
    diffSize=$((newSize - oldSize))

    if [[ "${sizeInMb}" -gt 1000 ]]; then
        if [[ "${diffSize}" -gt "1000000000" ]]; then
            # greater than 1 GB
            oldSize=${newSize}
            info "$((sizeInMb / 1000)) GB uploaded"
        fi
    elif [[ "${diffSize}" -gt "100000000" ]]; then
        # greater than 100 MB
        oldSize=${newSize}
        info "${sizeInMb} MB uploaded"
    fi
done

info "Cloud upload complete"

exit 0
